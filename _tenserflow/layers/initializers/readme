Glorot (Xavier):
    Use with activation functions like tanh or logistic sigmoid.
    It is designed to handle the vanishing/exploding gradient problems associated with these activation functions.

He:
    Use with Rectified Linear Unit (ReLU) and its variants.
    Helps prevent the vanishing gradient problem associated with ReLU activations.

LeCun:
    Historically used with tanh activation functions, but Glorot initialization is often preferred nowadays.
    Can still be useful in some cases.

Zeros:
    Rarely used for weight initialization in hidden layers due to symmetry issues.
    May be used for biases.

Ones:
    Rarely used for weight initialization in hidden layers due to symmetry issues.
    May be used for biases.

RandomNormal, RandomUniform, TruncatedNormal:
    Can be used for experimentation, but Glorot or He initializations are often more effective.

It's common practice to use Glorot initialization (glorot_normal or glorot_uniform) as a default choice for weight initialization in hidden layers. 